<!DOCTYPE html><html><head><style>body { margin: 0; font-family: system-ui, -apple-system, BlinkMacSystemFont, gotham, avenir, helvetica, ubuntu, roboto, noto, arial, sans-serif; } /* custom! */</style><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/writing/_next/static/3CQjeo_UXtpFrWg8sBpOH/pages/components-ai.js" as="script"/><link rel="preload" href="/writing/_next/static/3CQjeo_UXtpFrWg8sBpOH/pages/_app.js" as="script"/><link rel="preload" href="/writing/_next/static/runtime/webpack-a79426b5e11f0ba5879d.js" as="script"/><link rel="preload" href="/writing/_next/static/chunks/commons.5d95a14873fce403f456.js" as="script"/><link rel="preload" href="/writing/_next/static/runtime/main-c29dd18b0819bc878f70.js" as="script"/></head><body><div id="__next"><style data-emotion-css="1fe43t1">.css-1fe43t1{padding-bottom:64px;width:100%;width:100%;width:100%;width:100%;font-size:20px;font-size:20px;font-size:20px;color:inherit;box-sizing:border-box;}</style><div class="css-1fe43t1"><style data-emotion-css="1smxny8">.css-1smxny8{margin-left:auto;margin-right:auto;padding-left:16px;padding-right:16px;max-width:38em;width:100%;max-width:38em;width:100%;max-width:38em;width:100%;max-width:38em;width:100%;background-color:transparent;color:inherit;box-sizing:border-box;}</style><div class="css-1smxny8"><style data-emotion-css="zwemcz">.css-zwemcz{padding-top:64px;padding-bottom:32px;box-sizing:border-box;}</style><header class="css-zwemcz"><style data-emotion-css="wtna0o">.css-wtna0o{font-size:14px;font-size:14px;font-size:14px;}</style><time class="css-wtna0o">29-10-2019</time><style data-emotion-css="x42p9c">.css-x42p9c{margin-top:0;margin-bottom:0;font-size:20px;line-height:1.25;font-size:20px;line-height:1.25;font-size:20px;line-height:1.25;font-size:20px;line-height:1.25;box-sizing:border-box;}@media screen and (min-width:30em){.css-x42p9c{font-size:32px;}}@media screen and (min-width:60em){.css-x42p9c{font-size:48px;}}@media screen and (min-width:30em){.css-x42p9c{font-size:32px;}}@media screen and (min-width:60em){.css-x42p9c{font-size:48px;}}@media screen and (min-width:30em){.css-x42p9c{font-size:32px;}}@media screen and (min-width:60em){.css-x42p9c{font-size:48px;}}@media screen and (min-width:30em){.css-x42p9c{font-size:32px;}}@media screen and (min-width:60em){.css-x42p9c{font-size:48px;}}</style><h1 class="css-x42p9c">Components.ai</h1></header><style data-emotion-css="fetbeq">.css-fetbeq{margin-bottom:32px;max-width:38em;max-width:38em;max-width:38em;line-height:1.5;font-size:16px;line-height:1.5;font-size:16px;line-height:1.5;font-size:16px;line-height:1.5;font-size:16px;box-sizing:border-box;}@media screen and (min-width:30em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:60em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:30em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:60em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:30em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:60em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:30em){.css-fetbeq{font-size:20px;}}@media screen and (min-width:60em){.css-fetbeq{font-size:20px;}}</style><p class="css-fetbeq"><style data-emotion-css="16d3z1v">.css-16d3z1v{display:inline-block;display:inline-block;display:inline-block;background-color:transparent;box-sizing:border-box;-webkit-text-decoration:none;text-decoration:none;}.css-16d3z1v:hover{cursor:pointer;-webkit-transition:color .33s ease-in;transition:color .33s ease-in;opacity:1;color:#2967c3;}.css-16d3z1v:focus{cursor:pointer;-webkit-transition:color .33s ease-in;transition:color .33s ease-in;opacity:1;color:#3829c3;}</style><a href="https://twitter.com/mrmrs_/status/1145543229229735937" class="css-16d3z1v">In July</a>, <a href="https://johno.com" class="css-16d3z1v">John Otander</a> and I released a modest start to a project we are calling <a href="https://components.ai" class="css-16d3z1v">components.ai</a>. This is an ambitious and long-term project we&#x27;ve been working on aimed at exploring two questions: <br class="css-0"/><br class="css-0"/>In design, what can be computed?<br class="css-0"/>How might we contribute to our collective design knowledge?</p><p class="css-fetbeq">If one were to design a single static experience that was meant to work efficiently on all devices and accessible for everyone, you’d have millions of states to account for in your design. We are already far from full coverage on building fully accessible and responsive digital interfaces as it is. The problem space is already too large for our brains to fully wrap themselves around now. This will get worse over time as more contexts are introduced and the explosion of considerations and mediums that accompany designing for AR / VR / holographic vision. </p><p class="css-fetbeq">If we lived in a world where every possible iteration of a design was already built - what would your workflow be to define what you’d ship? What tools would you use to sift through all of this generated noise? A common critique around generative / procedural design output is that it’s too overwhelming. Too many choices. This is a valid consideration when thinking about the ergonomics of a tool or interface. But, through a different lens, the overwhelming amount of potential solutions exist, regardless if a piece of software is generating them or not. <a href="https://twitter.com/marian42_/status/1188969971898048512" class="css-16d3z1v">What do we need to make total generative space more manageable to explore? </a></p><p class="css-fetbeq">We think the future of design tooling will be centered around a search interface. One that allows you to filter or define the constraints and quickly cycle through available outputs.</p><p class="css-fetbeq">Currently, colors for our gradient components are generated from several algorithms and data sources. For serendipitous inspiration, you can cycle through attempts at the generators attempts at creating a pleasing gradient. Another mode to work in is to constrain the inputs with intent. Eliminating a certain range of hues or saturations you know to be off brand. Or defining an explicit list of brand colors to generate gradients from.  Maybe you need to dynamically generate gradients that go well with a photo’s color palette. </p><p class="css-fetbeq">No one wants to spend time reviewing design iterations that will never be implemented because they do not conform to a known constraint. Imagine only seeing designs that were visually accessible. Or that were themed with your brand colors. Or could handle low light and bright light situations. Constraints come in many forms - and our goal is to make design tools that allow you to search through generative space with any constraint you might have in mind. Using the history of css styles on the internet, you could generate design options to look most visually different than a defined list of sites. </p><p class="css-fetbeq">The starting point is small: design algorithms that style nodes using a variety of data sources. Over time as a designer gives feedback on the output, the density of high quality designs generated should increase for each component. As we evolve our understanding around styling single nodes in a desirable way, we can start to compose sets and seek to understand more complex relationships. As we learn more about what makes a pleasing gradient, we can use that data to inform our starting point when testing which gradients work well with buttons of various sizes. While a gradient might look wonderful as a full page background, it might look less appealing applied to an element the size of a button. Each component could be considered an encapsulated design research program.</p><p class="css-fetbeq">Within many of our components, we expect to find acceptable ranges for inputs and hope to externalize and document that data for public consumption.  As there is a range of acceptable measurements for the height of a stair,  what are the size dimensions and aspect ratios that determine where a button is too large or too small? If you’ve ever worked within a digital audio workstation, instruments and effects come with context labeled presets. What would presets for component design and theming options look like? Can they be established? </p><p class="css-fetbeq">Long-term we want to build a design tooling platform that allows you to generate, assess, and test designs at unprecedented scale. Whether it&#x27;s visualizing a wide variety of content choices, theming compositions, or layout options, it should be effortless to generate and explore design variants with your team. </p><p class="css-fetbeq">We believe that successful design in the future will necessitate creating things beyond our individual comprehension. This will require us to create more resilient and dynamic systems that learn and respond to feedback over time.</p><p class="css-fetbeq">We currently have a lot of questions but we’re excited and energized about the opportunity to explore potential answers. </p></div></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{}},"page":"/components-ai","query":{},"buildId":"3CQjeo_UXtpFrWg8sBpOH","dynamicBuildId":false,"assetPrefix":"/writing","runtimeConfig":{"linkPrefix":"/writing"},"nextExport":true}</script><script async="" id="__NEXT_PAGE__/components-ai" src="/writing/_next/static/3CQjeo_UXtpFrWg8sBpOH/pages/components-ai.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/writing/_next/static/3CQjeo_UXtpFrWg8sBpOH/pages/_app.js"></script><script src="/writing/_next/static/runtime/webpack-a79426b5e11f0ba5879d.js" async=""></script><script src="/writing/_next/static/chunks/commons.5d95a14873fce403f456.js" async=""></script><script src="/writing/_next/static/runtime/main-c29dd18b0819bc878f70.js" async=""></script></body></html>